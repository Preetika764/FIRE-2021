{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_submission.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH9BLQ7RvazZ"
      },
      "source": [
        "!pip install spacy\n",
        "!pip install urduhack\n",
        "!pip install transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification,AutoModel\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftCO73jq4xVk"
      },
      "source": [
        "Loading the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWzhKNY0wJZV"
      },
      "source": [
        "raw_datasets = load_dataset('csv', data_files={'train': '/content/train.csv',\n",
        "                                              'test': '/content/test.csv'})\n",
        "\n",
        "checkpoint = \"urduhack/roberta-urdu-small\" \n",
        "#checkpoint2 = \"xlm-roberta-base\" \n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer((example[\"Text\"]), truncation=True, padding=True,max_length=510)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "columns_to_return = ['input_ids', 'labels', 'attention_mask']\n",
        "tokenized_datasets.set_format(type='torch', columns=columns_to_return)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer,max_length=510)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeamXI5DuTeQ"
      },
      "source": [
        "raw_datasets = load_dataset('csv', data_files={'test1': '/content/test1.csv'})\n",
        "\n",
        "tokenized_datasets1 = raw_datasets.map(tokenize_function, batched=True)\n",
        "columns_to_return = ['input_ids','attention_mask']\n",
        "tokenized_datasets1.set_format(type='torch', columns=columns_to_return)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer,max_length=510)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AaTlc6VwTQ_"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxwSQdVfwe_4"
      },
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "from sklearn.metrics import f1_score,confusion_matrix,accuracy_score,roc_auc_score\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/FIRE2021/'\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = load_metric(\"accuracy\", \"f1\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained('/content/drive/MyDrive/FIRE2021/hyperparameter_tuning_roberta/checkpoint-1000')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHuFZzVYw1Bp"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    \"test-trainer\", evaluation_strategy ='steps', eval_steps=100)\n",
        "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy ='steps', per_device_train_batch_size = 2, per_device_eval_batch_size = 2,num_train_epochs=2,learning_rate=1.00742e-05,eval_steps=300)\n",
        "trainer = Trainer(\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test'],\n",
        "    model_init=model_init,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiCXp34kx2ob",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "d43f7ad5-cbc3-4c8a-a1db-54031769a53e"
      },
      "source": [
        "predictions = trainer.predict(tokenized_datasets['test'])\n",
        "preds = np.argmax(predictions.predictions, axis=-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 262\n",
            "  Batch size = 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='131' max='131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [131/131 00:20]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06Ugw75TgtLb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "0776383c-877d-4001-b306-1057cab7da9a"
      },
      "source": [
        "predictions1 = trainer.predict(tokenized_datasets1['test1'])\n",
        "preds1 = np.argmax(predictions.predictions, axis=-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 300\n",
            "  Batch size = 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='281' max='131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [131/131 01:19]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znPIbUKn5wLh"
      },
      "source": [
        "After doing hyperparameter tuning for Roberta-Urdu-small"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LctYqyNoGtmc",
        "outputId": "b6598c39-f6a2-4378-86bf-6e2b86a33f21"
      },
      "source": [
        "from datasets import load_metric\n",
        "metric1 = load_metric(\"accuracy\")\n",
        "print(metric1.compute(predictions=preds,references=predictions.label_ids))\n",
        "metric2 = load_metric(\"f1\")\n",
        "print(metric2.compute(predictions=preds,references=predictions.label_ids))\n",
        "metric5 = load_metric(\"f1\")\n",
        "print(metric5.compute(predictions=preds, references=predictions.label_ids, average='macro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 0.9083969465648855}\n",
            "{'f1': 0.9215686274509804}\n",
            "{'f1': 0.9057384421658572}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyfhaVZp57tf"
      },
      "source": [
        "raw_datasets = load_dataset('csv', data_files={'train': '/content/train.csv',\n",
        "                                              'test': '/content/test.csv'})\n",
        "\n",
        "#checkpoint = \"urduhack/roberta-urdu-small\" \n",
        "checkpoint = \"xlm-roberta-base\" \n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer((example[\"Text\"]), truncation=True, padding=True,max_length=510)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "columns_to_return = ['input_ids', 'labels', 'attention_mask']\n",
        "tokenized_datasets.set_format(type='torch', columns=columns_to_return)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer,max_length=510)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orMwh_h26EkK"
      },
      "source": [
        "raw_datasets = load_dataset('csv', data_files={'test1': '/content/test1.csv'})\n",
        "\n",
        "tokenized_datasets1 = raw_datasets.map(tokenize_function, batched=True)\n",
        "columns_to_return = ['input_ids','attention_mask']\n",
        "tokenized_datasets1.set_format(type='torch', columns=columns_to_return)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer,max_length=510)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb_iwKpfHWeG"
      },
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "from sklearn.metrics import f1_score,confusion_matrix,accuracy_score,roc_auc_score\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/FIRE2021/'\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = load_metric(\"accuracy\", \"f1\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained('/content/drive/MyDrive/FIRE2021/hyperparameter_tuning_xmlroberta/checkpoint-1000')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtvq26e-ZyPd"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    \"test-trainer\", evaluation_strategy ='steps', eval_steps=100)\n",
        "training_args = TrainingArguments(\"/content/drive/MyDrive/FIRE2021/hyperparameter_tuning_xmlroberta\", evaluation_strategy ='steps', per_device_train_batch_size = 4, per_device_eval_batch_size = 4,num_train_epochs=4,learning_rate=1.42903e-05,eval_steps=300)\n",
        "trainer = Trainer(\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test'],\n",
        "    model_init=model_init,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "cpHtl3eOZ1-Z",
        "outputId": "dad7d178-3e08-4180-985b-bf1fe18141d1"
      },
      "source": [
        "predictions = trainer.predict(tokenized_datasets['test'])\n",
        "preds = np.argmax(predictions.predictions, axis=-1)\n",
        "from datasets import load_metric\n",
        "metric1 = load_metric(\"accuracy\")\n",
        "print(metric1.compute(predictions=preds,references=predictions.label_ids))\n",
        "metric2 = load_metric(\"f1\")\n",
        "print(metric2.compute(predictions=preds,references=predictions.label_ids))\n",
        "metric5 = load_metric(\"f1\")\n",
        "print(metric5.compute(predictions=preds, references=predictions.label_ids, average='macro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 262\n",
            "  Batch size = 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [66/66 00:19]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 0.8053435114503816}\n",
            "{'f1': 0.8118081180811808}\n",
            "{'f1': 0.8051135452065983}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "NLc1XCQ2u-T2",
        "outputId": "90525caa-95ec-45e3-82bb-8b91d9e96376"
      },
      "source": [
        "predictions2 = trainer.predict(tokenized_datasets1['test1'])\n",
        "preds2 = np.argmax(predictions.predictions, axis=-1)\n",
        "print(preds2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 300\n",
            "  Batch size = 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='141' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [66/66 00:46]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
            " 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUaMSCCC67bt"
      },
      "source": [
        "Ensembling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOtlzs5ehNUy",
        "outputId": "bd1ed483-b230-41da-b8a9-9f1356cd590f"
      },
      "source": [
        "predictions3 = (predictions1.predictions + predictions2.predictions)/2\n",
        "preds3 = np.argmax(predictions3, axis=-1)\n",
        "print(preds3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnLIRPHviAMM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twzrvFz4iXl5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}